{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline: Hybrid Movie Recommendation System\n",
    "\n",
    "This notebook demonstrates the entire workflow:\n",
    "1.  **Setup**: Install dependencies and load config.\n",
    "2.  **Data**: Download and preprocess.\n",
    "3.  **Training**: Train Hybrid model and upload checkpoints to Hugging Face.\n",
    "4.  **Evaluation**: Evaluate model performance.\n",
    "5.  **Inference**: Generate recommendations with LLM explanation.\n",
    "6.  **Comparison**: SVD vs Hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4aacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Change to project root if in notebooks dir\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "load_dotenv()\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "CONFIG_PATH = \"config/config.yml\"\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(\"Config loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Download & Preprocess\n",
    "# Ensure kaggle.json is in place\n",
    "\n",
    "!python scripts/download_kaggle_datasets.py --config {CONFIG_PATH}\n",
    "\n",
    "# Preprocess\n",
    "from src.data.preprocess import Preprocessor\n",
    "\n",
    "preprocessor = Preprocessor(CONFIG_PATH)\n",
    "preprocessor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92665e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training & HF Upload\n",
    "# This script trains SVD + TF-IDF and uploads to xt2201/hybrid-movie-recsys\n",
    "\n",
    "!python scripts/train_hybrid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluation\n",
    "!python scripts/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inference with LLM\n",
    "from src.recommender.hybrid import HybridRecommender\n",
    "from src.llm.qwen_client import QwenClient\n",
    "from src.llm.reranker import Reranker\n",
    "from src.llm.explainer import Explainer\n",
    "\n",
    "# Load models\n",
    "recsys = HybridRecommender(CONFIG_PATH)\n",
    "recsys.fit() # Or load from checkpoint if implemented\n",
    "\n",
    "client = QwenClient(CONFIG_PATH)\n",
    "reranker = Reranker(client, CONFIG_PATH)\n",
    "explainer = Explainer(client, CONFIG_PATH)\n",
    "\n",
    "# User Query\n",
    "user_id = 1\n",
    "query = \"I want a touching drama about family\"\n",
    "\n",
    "# Get candidates\n",
    "raw_recs = recsys.recommend(user_id, N=20)\n",
    "candidates = []\n",
    "movies_df = recsys.dataset.movies\n",
    "\n",
    "for idx, score in raw_recs:\n",
    "    # Map internal idx to movie details\n",
    "    # Note: item_map is original_id -> internal_idx\n",
    "    # We need internal_idx -> details\n",
    "    # Find original ID\n",
    "    original_id = recsys.dataset.reverse_item_map[idx]\n",
    "    row = movies_df[movies_df['movieId'] == original_id].iloc[0]\n",
    "    candidates.append({\n",
    "        \"id\": idx,\n",
    "        \"title\": row['title'],\n",
    "        \"genres\": row['genres'],\n",
    "        \"overview\": row['overview'],\n",
    "        \"base_score\": score\n",
    "    })\n",
    "\n",
    "# Rerank\n",
    "prefs = {\"mood\": \"touching\", \"must_genres\": [\"Drama\"]}\n",
    "ranked = reranker.rerank(query, prefs, candidates)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top Recommendations:\")\n",
    "for item in ranked[:5]:\n",
    "    print(f\"- {item.get('title')} (Score: {item.get('score')})\")\n",
    "    print(f\"  Reason: {item.get('reason')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Comparison (SVD vs Hybrid)\n",
    "# We can plot metrics from W&B or just run quick eval here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dummy data for visualization if not running full eval loop again\n",
    "models = ['SVD', 'Hybrid']\n",
    "precision = [0.25, 0.28] # Example values\n",
    "recall = [0.20, 0.22]\n",
    "\n",
    "x = range(len(models))\n",
    "plt.bar(x, precision, width=0.4, label='Precision@10', align='center')\n",
    "plt.bar([i + 0.4 for i in x], recall, width=0.4, label='Recall@10', align='center')\n",
    "plt.xticks([i + 0.2 for i in x], models)\n",
    "plt.legend()\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
